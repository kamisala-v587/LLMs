{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6864a474",
   "metadata": {},
   "source": [
    "####  1. æ•°æ®å¤„ç†\n",
    "\n",
    "å¯ä»¥çœ‹åˆ°æ•°æ®æ˜¯å…¸å‹çš„BIOæ–‡ä»¶ï¼Œæ–‡æœ¬ç±»å‹ä¸ºtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e530f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_bio_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    label={}\n",
    "    \n",
    "    temp_label_name = \"\"\n",
    "    for line in lines:\n",
    "        # print(line)\n",
    "        line = line.replace('\\n','')\n",
    "        line = line.strip()\n",
    "        # print(line)\n",
    "        if not line:  # ç©ºè¡Œè¡¨ç¤ºå¥å­ç»“æŸ\n",
    "            if current_sentence:\n",
    "                # å¤„ç†æœ€åä¸€ä¸ªè¯\n",
    "                if len(temp_label_name)>0:\n",
    "                    label[temp_label_name] = temp_label_value\n",
    "                    temp_label_name=\"\"\n",
    "                sentences.append({'text':current_sentence,'label':label})\n",
    "                current_sentence = \"\"\n",
    "                label={}\n",
    "            continue\n",
    "        \n",
    "        parts = line.split()\n",
    "        current_sentence = current_sentence + parts[0]  # å­—ç¬¦\n",
    "        if len(parts)>=2 and parts[1] == 'O' and len(temp_label_name)>0:\n",
    "            label[temp_label_name] = temp_label_value\n",
    "            temp_label_name=\"\"\n",
    "        elif len(parts)>=2 and parts[1][0] =='B':\n",
    "            temp_label_name = parts[0]\n",
    "            temp_label_value =parts[1][2:]\n",
    "        elif len(parts)>=2 and parts[1][0] =='I':\n",
    "            temp_label_name = temp_label_name + parts[0]\n",
    "            \n",
    "    # å¤„ç†æœ€åä¸€ä¸ªå¥å­\n",
    "    if (current_sentence, label):\n",
    "        sentences.append({'text':current_sentence,'label':label})\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# ç¤ºä¾‹åŠ è½½\n",
    "train = load_bio_data('data/medical.train')\n",
    "dev = load_bio_data('data/medical.dev') # è·‘ä¸åŠ¨æˆ‘è¿™å›å°±ä¸ç”¨äº†\n",
    "test = load_bio_data('data/medical.dev') # è·‘ä¸åŠ¨æˆ‘è¿™å›å°±ä¸ç”¨äº†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dfabb8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'text': 'èˆ’è‚å’Œèƒƒæ¶ˆç—æ±¤ï¼›åŠŸèƒ½æ€§æ¶ˆåŒ–ä¸è‰¯', 'label': {'åŠŸèƒ½æ€§æ¶ˆåŒ–ä¸è‰¯': 'è¥¿åŒ»è¯Šæ–­'}}\""
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(train[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8be82bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_list(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8822e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åŸå§‹æ ·æœ¬é‡: 5259 ç¼©å‡å: 4879\n",
      "åŸå§‹æ ·æœ¬æœ€å¤§é•¿åº¦: 358 ç¼©å‡åæ ·æœ¬é•¿åº¦: 150\n"
     ]
    }
   ],
   "source": [
    "# ç¼©å‡æ•°æ®é•¿åº¦ åœ¨tokenizerä¸­é…ç½®max_length ä»¥å‡è½»è®­ç»ƒè´Ÿæ‹…\n",
    "train_150 = []\n",
    "max_len= 0 \n",
    "for i in train:\n",
    "    len_items = len(i['text']) +len(str(i['label'])) \n",
    "    max_len = max(max_len,len_items)\n",
    "    if len_items<=150:\n",
    "        train_150.append(i)\n",
    "print(\"åŸå§‹æ ·æœ¬é‡:\",len(train),\"ç¼©å‡å:\",len(train_150))\n",
    "print(\"åŸå§‹æ ·æœ¬æœ€å¤§é•¿åº¦:\",max_len,\"ç¼©å‡åæ ·æœ¬é•¿åº¦:\",150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fb42a31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ä¸­åŒ»è¯Šæ–­', 'ä¸­è¯', 'ä¸­åŒ»æ²»ç–—', 'æ–¹å‰‚', 'è¥¿åŒ»æ²»ç–—', 'è¥¿åŒ»è¯Šæ–­', 'ä¸­åŒ»æ²»åˆ™', 'ä¸­åŒ»è¯å€™', 'ä¸´åºŠè¡¨ç°', 'å…¶ä»–æ²»ç–—'}\n"
     ]
    }
   ],
   "source": [
    "# ç¼©å‡æ•°æ®é•¿åº¦ åœ¨tokenizerä¸­é…ç½®max_length ä»¥å‡è½»è®­ç»ƒè´Ÿæ‹…\n",
    "labels_name = set()\n",
    "for i in train:\n",
    "    for k in i['label']:\n",
    "        labels_name.add(i['label'][k])\n",
    "print(labels_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "06505584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import snapshot_download, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "# 1. åŠ è½½æ¨¡å‹\n",
    "model_id = \"qwen/Qwen2-1.5B-Instruct\"    \n",
    "model_dir = \"/Users/luyi/PythonProjects/Atomy/03LLMå¾®è°ƒ/å‘½åå®ä½“å¾®è°ƒå®æˆ˜/qwen/Qwen2-1___5B-Instruct\"\n",
    "\n",
    "# TransformersåŠ è½½æ¨¡å‹æƒé‡\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "# model.enable_input_require_grads()  # å¼€å¯æ¢¯åº¦æ£€æŸ¥ç‚¹æ—¶ï¼Œè¦æ‰§è¡Œè¯¥æ–¹æ³•"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee967714",
   "metadata": {},
   "source": [
    "#### æœªè®­ç»ƒå‰æµ‹è¯•ä¸€ä¸‹æ•ˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "bf838e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(messages, model, tokenizer):\n",
    "    \n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to('mps')\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        model_inputs.input_ids,\n",
    "        attention_mask=model_inputs.attention_mask, # åŠ è¿™ä¸€è¡Œé¿å…æŠ¥é”™ï¼šPlease provide an `attention_mask` or use a different device.\n",
    "        max_new_tokens=300\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    print(response)\n",
    "     \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d8745c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['èˆ’è‚å’Œèƒƒæ¶ˆç—æ±¤']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"['èˆ’è‚å’Œèƒƒæ¶ˆç—æ±¤']\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def testModel(model,instruction,input_value,tokenizer):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{input_value}\"}\n",
    "    ]\n",
    "    return predict(messages, model, tokenizer)\n",
    "instruction = \"\"\"ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å®ä½“è¯†åˆ«é¢†åŸŸçš„åŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦ä»ç»™å®šçš„å¥å­ä¸­æå–ä¸­åŒ»è¯Šæ–­,ä¸­è¯,ä¸­åŒ»æ²»ç–—, æ–¹å‰‚, è¥¿åŒ»æ²»ç–—, è¥¿åŒ»è¯Šæ–­ 'å…¶ä»–æ²»ç–—'ç­‰. ä»¥ json æ ¼å¼è¾“å‡º, å¦‚ {'å£è‹¦': 'ä¸´åºŠè¡¨ç°','è‚ºç»“æ ¸': 'è¥¿åŒ»è¯Šæ–­'} æ³¨æ„: 1. è¾“å‡ºçš„æ¯ä¸€è¡Œéƒ½å¿…é¡»æ˜¯æ­£ç¡®çš„jsonå­—ç¬¦ä¸². 2.æ‰¾ä¸åˆ°ä»»ä½•å®ä½“æ—¶, è¾“å‡º\"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“\".\"\"\"\n",
    "input_value = 'èˆ’è‚å’Œèƒƒæ¶ˆç—æ±¤ï¼›åŠŸèƒ½æ€§æ¶ˆåŒ–ä¸è‰¯'\n",
    "testModel(model,instruction,input_value,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ba764ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…æ‹¬ input_ids, attention_mask, labels\n",
    "# labelsä¸­çš„å€¼æ˜¯ input_ids ä¸­ attention_maskä¸º1 ç„¶å -100å¤„ç†å çš„å€¼\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 310 # 166 + 150 = 316 ç³»ç»Ÿæç¤ºé•¿åº¦ + æœ€é•¿çš„æ ·æœ¬é•¿åº¦\n",
    "    system_prompt = \"\"\"ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬å®ä½“è¯†åˆ«é¢†åŸŸçš„åŒ»å­¦ä¸“å®¶ï¼Œä½ éœ€è¦ä»ç»™å®šçš„å¥å­ä¸­æå–ä¸­åŒ»è¯Šæ–­,ä¸­è¯,ä¸­åŒ»æ²»ç–—, æ–¹å‰‚, è¥¿åŒ»æ²»ç–—, è¥¿åŒ»è¯Šæ–­ 'å…¶ä»–æ²»ç–—'ç­‰. ä»¥ json æ ¼å¼è¾“å‡º, å¦‚ {'å£è‹¦': 'ä¸´åºŠè¡¨ç°','è‚ºç»“æ ¸': 'è¥¿åŒ»è¯Šæ–­'} æ³¨æ„: 1. è¾“å‡ºçš„æ¯ä¸€è¡Œéƒ½å¿…é¡»æ˜¯æ­£ç¡®çš„jsonå­—ç¬¦ä¸². 2.æ‰¾ä¸åˆ°ä»»ä½•å®ä½“æ—¶, è¾“å‡º\"æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“\".\"\"\"\n",
    "    input_ids, attention_mask, labels = [], [], [] # æœ€ç»ˆè¿”å›çš„å­—å…¸å°±è¿™ä¸‰ä¸ªå€¼\n",
    "    \n",
    "    instruction = tokenizer(\n",
    "        f\"<|im_start|>system\\n{system_prompt}<|im_end|>\\n<|im_start|>user\\n{example['text']}<|im_end|>\\n<|im_start|>assistant\\n\",\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    response = tokenizer(f\"{example['label']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = (\n",
    "        instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]\n",
    "    )\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    if len(input_ids) > MAX_LENGTH:  # åšä¸€ä¸ªæˆªæ–­\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e74e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f27181224a954b8ab17c9b0bbede7296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4879 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.16s - Expected: /opt/anaconda3/envs/tunning/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd_attach_to_process/attach.dylib to exist.\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "train_ds = Dataset.from_list(train_150)\n",
    "train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c18fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¯¼åŒ…\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Loraå¾®è°ƒå‚æ•°\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    # target_modules=[\"q_proj\",  \"v_proj\", \"o_proj\", \"down_proj\"],\n",
    "    inference_mode=False,  # è®­ç»ƒæ¨¡å¼\n",
    "    r=8,  # Lora ç§©\n",
    "    lora_alpha=32,  # Lora alaphï¼Œå…·ä½“ä½œç”¨å‚è§ Lora åŸç†\n",
    "    lora_dropout=0.1,  # Dropout æ¯”ä¾‹\n",
    ")\n",
    "\n",
    "model_lora = get_peft_model(model, config)\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2-NER-Doctor\",\n",
    "    per_device_train_batch_size=4, # æ¯å¼ æ˜¾å¡ä¸Š çš„ batch_size ä¸€ä¸ªGPU å°±æ˜¯4\n",
    "    gradient_accumulation_steps=8, # ç´¯è®¡å¤šå°‘æ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°ï¼Œé€šå¸¸ä¼šæ¯”per_device_train_batch_sizeå¤§\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2,\n",
    "    save_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    # bf16=True,          # ğŸ‘ˆ å…³é”®ï¼M1 å¿…å¼€\n",
    "    # fp16=False,         # MPS ä¸æ”¯æŒ fp16ï¼Œå¿…é¡»ç”¨ bf16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb0508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#è®¾ç½® Trainer å¼€å§‹è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
