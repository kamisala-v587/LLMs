{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0324cb7a",
   "metadata": {},
   "source": [
    "本实验使用Qwen2.5-7B模型来进行未微调的基础测试"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174778b3",
   "metadata": {},
   "source": [
    "### 1. 数据处理 - Tokenizer\n",
    "#### 1.1 加载数据 - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f96d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "dev = pd.read_csv(\"data/dev.csv\")[:100] # 仅前300条数据用于测试\n",
    "dev_ds = Dataset.from_pandas(dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69d77d",
   "metadata": {},
   "source": [
    "#### 1.2 Tokenization\n",
    "+ 加载tokenizer \n",
    "+ 定义process function：prompt方程\n",
    "+ 处理dataset为 [input_id, attention_mask,labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba79a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoTokenizer\n",
    "model_dir =\"/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B/snapshots/d149729398750b98c0af14eb82c78cfe92750796\" # 定义本地路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True,padding_side='left') # 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义process function:\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 200\n",
    "    \n",
    "    instruction = \"\"\"你是一个文本实体识别领域的医学专家，你需要从给定的句子中提取中医诊断,中药,中医治疗, 方剂, 西医治疗, 西医诊断 '其他治疗'等. \n",
    "    注意: 1. 如果找到任何实体, 输出必须是严格的json字符串. 如 {'口苦': '临床表现','肺结核': '西医诊断'}。\n",
    "    2.找不到任何实体时, 输出\"没有找到任何实体\".\n",
    "    3.避免输出重复的内容\"\"\"\n",
    "    instructions_messages= [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{example['text']}\"}\n",
    "    ]\n",
    "\n",
    "    instructions_chatTamplate = tokenizer.apply_chat_template(instructions_messages, tokenize=True, add_generation_prompt=False,return_dict=True)\n",
    "\n",
    "    input_ids = instructions_chatTamplate['input_ids']\n",
    "    attention_mask = instructions_chatTamplate['attention_mask']\n",
    "    \n",
    "    # 限制最大长度做截断处理\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "    else:\n",
    "        pad_len = MAX_LENGTH - len(input_ids)\n",
    "        input_ids = [tokenizer.pad_token_id] * pad_len + input_ids\n",
    "        attention_mask = [0] * pad_len + attention_mask\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5f6073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1f3bf86bcd44d794f5ac2033401c77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dev_dataset = dev_ds.map(process_func, remove_columns=dev_ds.column_names,num_proc=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d18aef",
   "metadata": {},
   "source": [
    "### 2. 验证模型\n",
    "#### 2.1 加载原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21457f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61570e31f2048e8985cfea2fca0bf67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir, \n",
    "                                            torch_dtype=\"auto\",\n",
    "                                            device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fea2e1-5c57-4449-85d8-a3d555938e9e",
   "metadata": {},
   "source": [
    "测试原始模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f03be852-03a1-4f6c-80f9-7375e3b64429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def predict(dataset, model, tokenizer):\n",
    "    \"\"\"\n",
    "    修正后的预测函数：支持批量/单条样本，处理维度问题，优化切片逻辑\n",
    "    :param dataset: 处理后的Dataset（如dev_ds_processed[:5]）\n",
    "    :param model: 加载的LoRA/Qwen2模型\n",
    "    :param tokenizer: 初始化后的tokenizer\n",
    "    :return: 模型生成的实体识别结果列表\n",
    "    \"\"\"\n",
    "    # 步骤1：提取数据并转换为二维张量（适配批量输入）\n",
    "    input_ids = torch.tensor(dataset['input_ids']).to(model.device)\n",
    "    attention_mask = torch.tensor(dataset['attention_mask']).to(model.device)\n",
    "    \n",
    "    # 步骤2：模型生成（禁用梯度计算，节省显存）\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,  # 温度调到0\n",
    "            # do_sample=False,  # 关闭采样（此时temperature=0等价于贪心搜索）\n",
    "            repetition_penalty=1.2,  # 加大重复惩罚\n",
    "            # top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    # 步骤3：修正切片逻辑——去掉输入部分，仅保留模型生成的内容\n",
    "    # input_ids.shape[1]是单条样本的序列长度（MAX_LENGTH）\n",
    "    input_seq_len = input_ids.shape[1]\n",
    "    generated_ids = generated_ids[:, input_seq_len:]  # 批量切片\n",
    "    \n",
    "    # 步骤4：解码结果（跳过特殊token，得到纯净文本）\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)    \n",
    "    # # 打印结果\n",
    "    # for idx, res in enumerate(response):\n",
    "    #     print(f\"样本{idx+1}生成结果：\\n{res}\\n\" + \"-\"*80)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48180ab7-bf87-480f-a94f-2f6434774c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re,ast\n",
    "def f1_helper(response):\n",
    "    dataset = {}\n",
    "    for idx,res in enumerate(response):\n",
    "        if idx == 16:\n",
    "            pass\n",
    "        pattern = r\"\\{[\\s\\S]*?\\}\"  # 匹配JSON大括号片段\n",
    "        res = re.search(pattern, res)\n",
    "        \n",
    "        res = res.group() if res else \"{}\"\n",
    "        \n",
    "        label = dev['label'][idx]\n",
    "        # print(res,label)\n",
    "        try:\n",
    "            res = ast.literal_eval(res)\n",
    "            res = res if isinstance(res, dict) else {}\n",
    "        except:\n",
    "            res = {}\n",
    "        label = ast.literal_eval(label) # k 是 '腹痛'症状 v 是 '临床表现'\n",
    "\n",
    "\n",
    "        # 步骤3：正确计算键集（修复核心逻辑颠倒问题）\n",
    "        label_keys = set(label.keys())\n",
    "        res_keys = set(res.keys())\n",
    "\n",
    "        samekeys = label_keys & res_keys  # 标签和预测都有的键（实体匹配）\n",
    "        diffkeys1 = label_keys - res_keys  # 标签有、预测无 → 漏标（FN）\n",
    "        diffkeys2 = res_keys - label_keys  # 预测有、标签无 → 误标（FP）\n",
    "\n",
    "        # 处理相同的\n",
    "        for k in samekeys:\n",
    "            v1 = label[k]\n",
    "            v2 = res[k]\n",
    "            if v1 not in dataset:\n",
    "                dataset[v1] = {\"FN\":0,\"FP\":0,\"TP\":0} \n",
    "            if v1 == v2:\n",
    "                dataset[v1][\"TP\"]+=1\n",
    "            elif v1!=v2:\n",
    "                dataset[v1][\"FP\"]+=1\n",
    "                if v2 in dataset:\n",
    "                    dataset[v2][\"FN\"]+=1\n",
    "        for k in diffkeys1:\n",
    "            v1 = label[k]\n",
    "            if v1 not in dataset:\n",
    "                dataset[v1] = {\"FN\":0,\"FP\":0,\"TP\":0} \n",
    "            dataset[v1][\"FP\"]+=1\n",
    "        for k in diffkeys2:\n",
    "            v2 = res[k]\n",
    "            if isinstance(v2, str) and v2 in dataset:\n",
    "                dataset[v2][\"FN\"]+=1\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d60f7c47-9859-4bb2-a7c2-ed38b938233d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(metric_dict: dict, key: str) -> float:\n",
    "    \"\"\"\n",
    "    根据实体类型的TP/FP/FN字典，输入key计算对应F1分数\n",
    "    :param metric_dict: 包含各实体类型TP/FP/FN的字典（如你提供的字典）\n",
    "    :param key: 要查询的实体类型key（如'中药'/'西医诊断'）\n",
    "    :return: 该类别的F1分数（保留4位小数）\n",
    "    \"\"\"\n",
    "    # 检查key是否存在于字典中\n",
    "    if key not in metric_dict:\n",
    "        raise KeyError(f\"Key '{key}' 不存在于字典中，可选key: {list(metric_dict.keys())}\")\n",
    "    \n",
    "    # 提取该key对应的TP、FP、FN\n",
    "    tp = metric_dict[key]['TP']\n",
    "    fp = metric_dict[key]['FP']\n",
    "    fn = metric_dict[key]['FN']\n",
    "    \n",
    "    # 计算精确率（处理分母为0的情况）\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "    # 计算召回率（处理分母为0的情况）\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    # 计算F1分数（处理分母为0的情况）\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    \n",
    "    # 保留4位小数，便于阅读\n",
    "    return round(f1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467fb8f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中药的F1分数：0.3288\n",
      "西医诊断的F1分数：0.0727\n",
      "临床表现的F1分数：0.0645\n"
     ]
    }
   ],
   "source": [
    "# response_base = predict(dev_dataset, model, tokenizer)\n",
    "dataset_base = f1_helper(response_base)\n",
    "# 示例1：查询'中药'的F1分数\n",
    "f1_herb = get_f1_score(dataset_base, '中药') # 0.2353\n",
    "print(f\"中药的F1分数：{f1_herb}\")\n",
    "\n",
    "# 示例2：查询'西医诊断'的F1分数\n",
    "f1_west_diag = get_f1_score(dataset_base, '西医诊断') # 0.2667\n",
    "print(f\"西医诊断的F1分数：{f1_west_diag}\")\n",
    "\n",
    "# 示例3：查询'临床表现'的F1分数\n",
    "f1_symptom = get_f1_score(dataset_base, '临床表现') # 0.0\n",
    "print(f\"临床表现的F1分数：{f1_symptom}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557fd2d1-2168-45e3-9c66-be9264e8bb02",
   "metadata": {},
   "source": [
    "### 2.2 加载Lora参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25765312-a37b-4dcc-8b65-23a93899c0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 7,635,801,600 || trainable%: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "# 加载LoRA权重（轻量化推理）\n",
    "peft_model = PeftModel.from_pretrained(model, \"./lora_weights_qwen2.5_7B\", inference_mode=True)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67b361ab-c0b1-42a6-a16d-c11470017cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing step: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing step: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"�assistant\\n{'活络效灵丹加味': '方剂', '当归': '中药', '丹参': '中药', '生乳香': '中药', '生没药': '中药', '柴胡': '中药', '黄芩': '中药', '大黄': '中药', '蒲公英': '中药', '甘草': '中药'}�\\n�assistant\\n{'活络效灵丹加味': '方剂', '当归': '中药', '丹参': '中药', '生乳香': '中药', '生没药': '中药', '柴胡': '中药', '黄芩': '中药', '大黄': '中药', '蒲公英': '中药', '甘草': '中药'}�\\n�assistant\\n{'活络效灵丹加味': '方剂', '当归': '中药', '丹参': '中药', '生乳香': '中药', '生没药': '中药', '柴胡': '中药', '黄芩': '中药', '大黄': '中药', '蒲公英': '中药', '甘草': '中药'}�\\n�assistant\\n{'活络效灵丹加味': '方剂', '当归': '中药', '丹参': '中药', '生乳香': '中药', '生没药': '中药', '柴\",\n",
       " \"�assistant\\n{'糖尿病性功能性消化不良': '西医诊断'}�\\n�assistant\\n{'糖尿病性功能性消化不良': '西医诊断'}�\\n�assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}�\\n�assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}�\\n�assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}�\\n�assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}�\\n�assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}�始化\\n１０assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}\\tTokenNameIdentifier\\n thuisontvangstassistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'} ForCanBeConverted\\nuseRalassistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'} ForCanBeConvertedToF\\n２０assistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}PostalCodesNL\\n$PostalCodesNLassistant\\n{'糖尿病性功能性消化不良': '西医诊断', '西沙必利片': '西医治疗'}\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_lora = []\n",
    "x = 0\n",
    "while x <len(dev_dataset):\n",
    "    if x % 60 ==0:\n",
    "        print(\"processing step:\", x)\n",
    "    response = predict(dev_dataset[x:x+20], peft_model, tokenizer)\n",
    "    response_lora +=response\n",
    "    x  = x +20\n",
    "    \n",
    "response_lora[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6647d9ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中药的F1分数：0.8521\n",
      "西医诊断的F1分数：0.84\n",
      "临床表现的F1分数：0.8148\n"
     ]
    }
   ],
   "source": [
    "# response_lora = predict(dev_dataset, peft_model, tokenizer)\n",
    "\n",
    "dataset_lora = f1_helper(response_lora)\n",
    "# 示例1：查询'中药'的F1分数\n",
    "f1_herb = get_f1_score(dataset_lora, '中药')\n",
    "print(f\"中药的F1分数：{f1_herb}\")\n",
    "\n",
    "# 示例2：查询'西医诊断'的F1分数\n",
    "f1_west_diag = get_f1_score(dataset_lora, '西医诊断')\n",
    "print(f\"西医诊断的F1分数：{f1_west_diag}\")\n",
    "\n",
    "# 示例3：查询'临床表现'的F1分数\n",
    "f1_symptom = get_f1_score(dataset_lora, '临床表现')\n",
    "print(f\"临床表现的F1分数：{f1_symptom}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be252bd-6dc8-406a-8279-b5de4f29ba18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
