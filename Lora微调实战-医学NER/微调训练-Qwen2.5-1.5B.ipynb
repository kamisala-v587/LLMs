{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0324cb7a",
   "metadata": {},
   "source": [
    "本实验使用Qwen2.5-1.5B模型 测试\n",
    "仅用于学习不用于生成最终模型 - 因此数据量也较低"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174778b3",
   "metadata": {},
   "source": [
    "### 1. 数据处理 - Tokenizer\n",
    "#### 1.1 加载数据 - Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0f96d02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['现头昏口苦',\n",
       "  '目的观察复方丁香开胃贴外敷神阙穴治疗慢性心功能不全伴功能性消化不良的临床疗效',\n",
       "  '舒肝和胃消痞汤；功能性消化不良',\n",
       "  '患者３ａ前咯血，被诊断为肺结核，住院４０余天时出现腹痛，经治疗好转，但时有发作，坚持服抗痨药３ａ后，因腹痛基本缓解，肺结核治愈而停药',\n",
       "  '治疗组采用复方蜥蜴散不同微粒组合剂（密点麻蜥、炙黄芪、焦乌梅、炒白芍、三七、半枝莲等）治疗'],\n",
       " 'label': [\"{'口苦': '临床表现'}\",\n",
       "  \"{'复方丁香开胃贴': '中医治疗', '心功能不全伴功能性消化不良': '西医诊断'}\",\n",
       "  \"{'功能性消化不良': '西医诊断'}\",\n",
       "  \"{'咯血': '临床表现', '肺结核': '西医诊断'}\",\n",
       "  \"{'复方蜥蜴散': '方剂', '密点麻蜥': '中药', '炙黄芪': '中药', '焦乌梅': '中药', '炒白芍': '中药', '三七': '中药', '半枝莲': '中药'}\"]}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "train = pd.read_csv(\"data/train.csv\")[:2000]\n",
    "train_ds = Dataset.from_pandas(train)\n",
    "train_ds[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f69d77d",
   "metadata": {},
   "source": [
    "#### 1.2 Tokenization\n",
    "+ 加载tokenizer \n",
    "+ 定义process function：prompt方程\n",
    "+ 处理dataset为 [input_id, attention_mask,labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba79a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope import AutoTokenizer\n",
    "model_dir =\"/root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323\"  # 定义本地路径\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, use_fast=False, trust_remote_code=True) # 加载tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bac315bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义process function:\n",
    "def process_func(example):\n",
    "    MAX_LENGTH = 300\n",
    "    \n",
    "    instruction = \"\"\"你是一个文本实体识别领域的医学专家，你需要从给定的句子中提取中医诊断,中药,中医治疗, 方剂, 西医治疗, 西医诊断 '其他治疗'等. 以 json 格式输出, 如 {'口苦': '临床表现','肺结核': '西医诊断'} 注意: 1. 输出的每一行都必须是正确的json字符串. 2.找不到任何实体时, 输出\"没有找到任何实体\".\"\"\"\n",
    "    instructions_messages= [\n",
    "        {\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{example['text']}\"}\n",
    "    ]\n",
    "    response_messages=[\n",
    "            {\"role\": \"assistant\", \"content\":f\"{example['label']}\"},\n",
    "        ]\n",
    "    # Chat-template + tokenizer 处理\n",
    "    instructions_chatTamplate = tokenizer.apply_chat_template(instructions_messages, tokenize=True, add_generation_prompt=False,return_dict=True)\n",
    "    total_chatTamplate = tokenizer.apply_chat_template(instructions_messages + response_messages, tokenize=True, padding=True,truncation=True,max_length=MAX_LENGTH,add_generation_prompt=False,return_dict=True)\n",
    "\n",
    "    # 凑labels的内容\n",
    "    instruction_len = len(instructions_chatTamplate['input_ids'])\n",
    "    labels = [-100] * instruction_len + total_chatTamplate['input_ids'][instruction_len:]\n",
    "    input_ids = total_chatTamplate['input_ids']\n",
    "    attention_mask = total_chatTamplate['attention_mask']\n",
    "    \n",
    "    # 限制最大长度做截断处理\n",
    "    if len(input_ids) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    \n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c5f6073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0cf105de67a4b158a67f55e82cd2c32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_ds.map(process_func, remove_columns=train_ds.column_names,num_proc=4) \n",
    "# 删除column_names 保证简洁性\n",
    "# 多线程处理数据 加速数据处理速度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d18aef",
   "metadata": {},
   "source": [
    "### 2. 训练模型\n",
    "#### 2.1 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21457f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "model.enable_input_require_grads()  # 开启梯度检查点时，要执行该方法 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91155c0",
   "metadata": {},
   "source": [
    "#### 2.2 定义Lora参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00dc387a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导包\n",
    "import torch\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Lora微调参数\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, # 实现方式为生成式，因此选CAUSAL_LM 不选 TOKEN_CLASSIFICATION\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, \n",
    "    r=8, \n",
    "    lora_alpha=32,  \n",
    "    lora_dropout=0.1,  \n",
    ")\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00eff662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 9,232,384 || all params: 1,552,946,688 || trainable%: 0.5945\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters() # 检查模型可训练参数大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6686d087",
   "metadata": {},
   "source": [
    "### 2.3 定义Trainer参数 并开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f89473b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./output/Qwen2-NER-Doctor\", # 模型输出地址\n",
    "    per_device_train_batch_size=4, # 每张显卡上 的 batch_size 一个GPU 就是4\n",
    "    gradient_accumulation_steps=8, # 累计多少步更新一次参数，通常会比per_device_train_batch_size大\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=2, # 训练的轮次\n",
    "    save_steps=50,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=False, # fp16需要N卡\n",
    "    bf16=True, # mps 仅支持高精度训练 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "171fbd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='126' max='126' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [126/126 03:41, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.376300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.892600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.795800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.805000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.733600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.753200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.752400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.734800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.719800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=126, training_loss=0.8041241433885362, metrics={'train_runtime': 223.2181, 'train_samples_per_second': 17.92, 'train_steps_per_second': 0.564, 'total_flos': 9500925542400000.0, 'train_loss': 0.8041241433885362, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#设置 Trainer 开始训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=\"max_length\",max_length=300)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77482935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存LoRA权重（仅几MB~几十MB，无需保存完整大模型）\n",
    "trainer.model.save_pretrained(\"./lora_weights_qwen2.5_1.5B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e316594",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
